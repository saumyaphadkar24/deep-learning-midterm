{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70hrNJwhYMjR"
   },
   "source": [
    "# Math Question Answer Verification Competition\n",
    "\n",
    "## Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp8dK32_gOZu"
   },
   "source": [
    "Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bA1lW9pzWwpk",
    "outputId": "48a99073-2250-4cba-a3ba-14eeb829050e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Using cached unsloth-2024.11.7-py3-none-any.whl.metadata (59 kB)\n",
      "Requirement already satisfied: unsloth-zoo>=2024.11.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.11.5)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.28.post3)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.1)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.8.14)\n",
      "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.46.2)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.1.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.12.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.26.2)\n",
      "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (0.20.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
      "Using cached unsloth-2024.11.7-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.11.7\n",
      "Found existing installation: unsloth 2024.11.7\n",
      "Uninstalling unsloth-2024.11.7:\n",
      "  Successfully uninstalled unsloth-2024.11.7\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-mila2y0o/unsloth_7297e1338e3243288242c3dd6c5202c8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-mila2y0o/unsloth_7297e1338e3243288242c3dd6c5202c8\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit f26d4e739ed507de7a9088da53d10fd02f58d160\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: unsloth-zoo>=2024.11.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
      "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.2)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.2)\n",
      "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.3)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.12.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2024.11.7-py3-none-any.whl size=163138 sha256=de5fc393bef514ef2ec8fbf178837cbc4f2980e8c684e654aabc0138648e57ec\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jbcvuisf/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.11.7\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# This cell will take time\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from google.colab import drive\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlpjJOhtW7g3",
    "outputId": "558f781d-3b01-410f-a1f4-f802dbe97d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 # Choose any\n",
    "# Specify dtype based on GPU type. Use None for automatic detection.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473,
     "referenced_widgets": [
      "43639df157ec46ea9966caa31d3232a1",
      "aa5222d4f4ef4f5197fd2659093f2df3",
      "682dcd7fed43436c9226f69c351d0a5b",
      "2203174f60fc40ad817d7f23ee2ff71f",
      "2a15759f37e54e3281ddf7fe21f0412c",
      "e1bf5331aaba46c492e3fade22998584",
      "a2aac5c7986b4a5ba45ca44288bcfc11",
      "131072079a7d4f35b94c730bc4c8e7b0",
      "ea2c0adeacb34c7193aa2498cde2198d",
      "91a5d8ce958f42429449b92333f2e5fa",
      "5f7bf3e208ac4a298170dc131648b2d5",
      "2c3c553d5abe48478bb810ccd3557c0b",
      "6a8780b1224248188d276ae54fa16d98",
      "57f6348e5396433bbe5fecd6d033d90a",
      "cb7a32b1b2744e5fb60ac5f894f50d22",
      "41dbd4f945ac4359bb3f5689524bd4f9",
      "7fd66f8d9ac34eedb40f89a77663d7ba",
      "9932e5f631fa4389afc8e557216df6a2",
      "e5fafef71be7444e8d988c858486a542",
      "1eca02db802f4859ba3924b65f681ea8",
      "d934112677bf477f9a05eb820e11814e",
      "d8dd0f8cff264e4d995f1c26e525426b",
      "699ddf4640f1413d980298a358a9255c",
      "cb417696161a4ebaaa3ed9540e7f8d8e",
      "f7b69c96c1694c288ed0e47b5c244b63",
      "ab8da48e90494531a4d9225a7b356d32",
      "7ef4f0fb51ae4f57aa38ae2b295b1f01",
      "0309ff08f30b48488b67ce108da889d1",
      "87e9de10cb7d4912b36ecc90961cb129",
      "eb94a1a57fbd44a483011dce3e4d19a9",
      "7a2a114adb6540898db81204b745ea9a",
      "d5a6d99237a747c7893115af30a34f53",
      "c8490d5e649a4fe8bd5f2f99f742aa2b",
      "ee450380eb614652b81e9aac79684c00",
      "e104ed64d4694156b3538084f39ba189",
      "2b9a7c1654ae42478a742a664de08c10",
      "208a4c216cfb40f1a5fc0ca71d0e333e",
      "012183febf104c32b53a3600bd709aea",
      "a2ab5295e19241aea5ed83dd4a8bca08",
      "986710c066484cd88b090247cf3bb5ea",
      "36c6e60057924273934a5db5df5f4f5c",
      "98c9343de2df45c3adc184f612f29bd4",
      "2462b1286b0845edb253f8751a28017c",
      "7fd8e3c1787145ddb63d3af1a02b6ed1",
      "20d054633eaf4d2bada1aa6bee2bc43c",
      "51e342132721471dabdaf908028ce3e8",
      "d5f4d6d8ffc54579b711c9cca617763c",
      "804a0a2011f74bbb95106dc3787f9cf9",
      "dc0bd2a230a144d0b6b141e106de115c",
      "8eff48c1a77347e6aa1ecd4beab84fc3",
      "216ae7ff426c4737bf4bff01dde1747a",
      "bfa6c214f1c74143b96d0d6d15da3c1e",
      "3d10ac754529469283c33548d0dcdb58",
      "0f1ddccff3104ec29c58e7252e4cf512",
      "985d1ead1b524d1f8fabf24458698959",
      "f1dc7ce98c01411a8abbecf898318212",
      "c733aeb49ceb4a298e7f7173956044be",
      "2374ea6820474379b99fa03532632499",
      "458dedbdb2af4e1a9dbcdf5688db543c",
      "dd7b6040c790466186eb4830b7ea1bcb",
      "4b22c52d17f440dab8aeba6185074b9d",
      "090fe56c81a8453186b8ea1127f1a76e",
      "b7dee85e45094050aa69fd9c2013f9ab",
      "fca247bec7df40e48e2fadd9adbfb951",
      "cabf9b5f5be44f7b8efa9fc0e2ca6321",
      "15038bbc5d284d83977693700079ec76",
      "ee607e3e8a5a4dd888a924e347ac234b",
      "248c19541f4e4a739719c36e7f6bdc8e",
      "9ec3cb2f8f9b46138306c87959704ee3",
      "5c665209cd744ecc95fbd97faa259a04",
      "7fa9175d1e2f47a2a38ef6f41d4a370d",
      "bdccdb46d49f44069d87caa6cd58f565",
      "7d8dce4a658048dab6670de4e378889f",
      "0ecc8f1746cc4b0a92d9ca1b27ee4c21",
      "9e017beedfab49639d8a2c46570bbda0",
      "684d71411b1e4cd8b4ee2853628074f0",
      "4bf3be448f0343c39a9e8a6a07f3d3ec",
      "e4a5c723cb7c4e99bd6cf039f2015f53",
      "d4dded3fceb24dd89c5a7a045af6a8d0",
      "d392ef55e1634230b8b0097044b78a55",
      "2ffee8f16bd24fd5add45150d4e22a62",
      "d831508e5c494d8ab8bd70005f3dffec",
      "d18a72030ffc4de3b25e9bb6163b0021",
      "e3931946f8094b38bb7d39bc8f190525",
      "6794b348b3c541909d6e3648d8f6990c",
      "37d7f490192d4545af35d711b3d9a37f",
      "8f8d73673e2c4c4fa11cada137d626e2",
      "dbc09593f8f84d5b9ee4e679f25bc04f",
      "fe0a3fda8bf74527afb8a3ff030d1850",
      "b063837f53db44edbffa23230dbaedbc",
      "d2c3547cba3c4164a8ac409fd802f128",
      "702111784c464dbda7ea6d8b270bc286",
      "b357eab730294648a19cc4d409000c6b",
      "b712ddcfef82483cb6c358ed8faba3bb",
      "c65758aef60a4461946853d2c3dc3f4f",
      "6b869791a81c4d26a95cf97e658ae20b",
      "8b7e6517f5854c8bba5a87fae483c7c7",
      "44033c2a0acf4e8db09f8749270ef735",
      "e36c54e3a62b432bb3fdc75a07fb5392",
      "fdf278c3a79b42269c627093b445c0fb",
      "d6e6eebeb08443afb10ac39ba8fd9b11",
      "89c45363368d4bb69541e5db669f52dc",
      "51d81c802e7a4199a110a1ae88f63f78",
      "0c7fe27642c44c8685673b045a5af665",
      "b3c20fb7be534fc198e38aad7a42a46b",
      "f9bf6a9146f344e098e8a14e8a404230",
      "758f0ef552b14f269ccadfdb4755efc5",
      "1f9ccda4a4d54bd488265a1c4918a54a",
      "79f873e81ac744c7a8b4598d89084672",
      "55161ef18ef04f4f8cc9b21c3c5c9ba5",
      "1578a25a1ed645098931bb8833b2f9d4",
      "b16854a91cab4c68b9d3eab11d513999",
      "3b8cf07d30c243d19207751a7a23a3b0",
      "c1f8567fa50f45b38e225b64b8a4ef26",
      "1ce813997010498fa1be30c34b654137",
      "544df810527d47449cb2525fc8abe84d",
      "e9441a35e41d448d81359ca7461e35ae",
      "90225350f6604a979e03fe38340714a1",
      "0b0fbdfd90c84dd78163ce5558fd91e2",
      "d250a3e8e7d843a5be194b355aa06f85",
      "e672a2845e1e48b2b61fd935b997c12a"
     ]
    },
    "id": "5GxOyBTkXJIG",
    "outputId": "3690d8d9-186c-49c7-c3f6-df74e03b4c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43639df157ec46ea9966caa31d3232a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3c553d5abe48478bb810ccd3557c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699ddf4640f1413d980298a358a9255c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee450380eb614652b81e9aac79684c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d054633eaf4d2bada1aa6bee2bc43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dc7ce98c01411a8abbecf898318212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee607e3e8a5a4dd888a924e347ac234b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a5c723cb7c4e99bd6cf039f2015f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0a3fda8bf74527afb8a3ff030d1850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf278c3a79b42269c627093b445c0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1578a25a1ed645098931bb8833b2f9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVgabGjM8G1r"
   },
   "source": [
    "## Load model and wrap with LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xy0iN0RJXMAX",
    "outputId": "c4578c0f-a2c9-4481-94f8-1b8ab1f7059f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth: bias = `none` is supported for fast patching. You are using bias = all.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.11.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n",
    "    bias = \"all\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNruHjDieGSS"
   },
   "source": [
    "## Competition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "fc0e2a7ae7d440538d4ab7fcf861c8a7",
      "df3393a49e3c43bea0368992e321d9a7",
      "f1354b6f8ceb48cb81ad23e6cc8bd1b5",
      "c3121ca93085411185ab622a31384e7f",
      "c205f8524952461085e3cad90df57ef7",
      "c6f9fdfac8a0477a8d520494e825677f",
      "882d8b2d61a3411d820f876c5a2b0699",
      "0135e60e3d28487a93f6d77fe060f61b",
      "36b50f4238554f628d5d5de6845ee23d",
      "cdb710d9b0de4f22900856006a61e75f",
      "e014af0d7a1046c8afe1d7d1bb9c692c",
      "a3943995421e422880fb951398191176",
      "7e7bfc1c1b564a02be8089e63de8e6d3",
      "0af79b04787d46b7bb8aae1460fa0646",
      "07ef6029c0cf4a4799afb85074575ba1",
      "732d5c317c2841a1bafa69688efeaed1",
      "4d369ea873a143378aeb9111661f4150",
      "f50a2c8ac0804342a7a8f529954e54d2",
      "92f9b45b509e43588044391f888cc610",
      "8e75a341114f43498b56d2009ec679bf",
      "d9fc26550ffc4d36a78991dcbcedcf0c",
      "9fdb204ba601418e97eaf58afbe274cc",
      "b848d512be124ec682f52a739a412f25",
      "776b2d4715914e0bb0ae3478a9755a7a",
      "b8596187477d4158be151c23667b1d7e",
      "3f9e8c41c7fe4f0d950308799d2ad77b",
      "e30c8f5947a24f36820e8d3eca1c81ed",
      "df78ef89f9aa48d7ad932a107fc385d5",
      "f7cc47d368be47529c3b99e279f26380",
      "a26564e4afda443984320b1af1d98d3c",
      "b54408405a8944f6b28527603cc0d493",
      "d17a133d99fc4eae8f6edc098f2907cb",
      "afc9238cd91e41c788a2ed46d725cedb",
      "40a991b4827e415abafd4d2ead78e15b",
      "60f943f9cd2e42e3a9c9550ec6250f02",
      "4dd15f1dda674aaf842d82e0e7426146",
      "3af119a476ed425ea3c2b1f35dc76006",
      "73cb14bff6b94bfaaa10ff8322321585",
      "e9e440da2b0249ad9fe32eb320b7ed98",
      "23c4175f2f974560bd2f1428d55d5600",
      "20382fc71f8247ca873a1e7bacc49e9a",
      "fdf7779d9fd64c8ebf7dede81da19922",
      "da53f32230b84d48ab2725add7b9f26c",
      "4faf8ff680c246a2b92f518d72968db4",
      "fa12ae7a07ee443b88e78c24747a1275",
      "ec4f2280943e48f280e4e3f6b18bd8bc",
      "50c9db97ce3f4012b96595dda7adbb06",
      "c721776dc43d49c28f8644a9d6d26e6d",
      "893616e095264a9f9bd7774e941fd5bb",
      "a77ceb6ea03a4b969380c78d965e0248",
      "b09cc92770cf4bf89497bdb8fdb9495b",
      "fb7a2756c4e840bcb3fd9e449a879619",
      "d26cd6de0fb44615a0ab6d3fc7831505",
      "90b692fe95554b74bc92a1c70895ca24",
      "8200ea0134fe474bb21b9c17f38e42ab",
      "e8bf62403c4642aba4a4ac63cbe22fb9",
      "2c13233e89ba4c09a073774f4dfb2a73",
      "e18eb72a61ad4ec39213a2c869f0734e",
      "6763b26250334031871655a9ca2eafcc",
      "359b736af4584dea9f8d58b13c7f2bdb",
      "36064c8a8b7444cb8db011ec6f2c10bf",
      "e97a704208504ddfafc7cd38ae112dc0",
      "862642e7ea35490796b3049a83451636",
      "b52cd4d0956b4cb49c579a009712b727",
      "9db68e6edc3a4c779ef27ef5e6c199af",
      "11f11e4ac409479e837334167b2ab453"
     ]
    },
    "id": "3OMXJz4Z8jhJ",
    "outputId": "ec5c9fc4-85bf-4d63-fa79-8eea82620f9f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0e2a7ae7d440538d4ab7fcf861c8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3943995421e422880fb951398191176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b848d512be124ec682f52a739a412f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a991b4827e415abafd4d2ead78e15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa12ae7a07ee443b88e78c24747a1275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bf62403c4642aba4a4ac63cbe22fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download and load competition dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
    "# print and see dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBpDwJA-bJ9K"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Review the provided math question, its solution, and the final answer. Verify the accuracy of the solution steps and the correctness of the final answer based on the calculations provided.\n",
    "Please analyze the information below and explicitly state 'True' if the solution accurately solves the problem and the answer matches, or 'False' if it does not. Do not provide additional information or explanation in your response.\n",
    "\n",
    "### Math Question:\n",
    "{}\n",
    "\n",
    "### Provided Answer:\n",
    "{}\n",
    "\n",
    "### Detailed Solution:\n",
    "{}\n",
    "\n",
    "Answer is correct (True/False only):{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    question = examples[\"question\"]\n",
    "    answer = examples[\"answer\"]\n",
    "    solution = examples[\"solution\"]\n",
    "    output = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "\n",
    "    for ques, ans, sol, out in zip(question, answer, solution, output):\n",
    "        # Format the prompt and add the \"text\" field\n",
    "        text = prompt.format(ques, ans, sol, out) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoVVYK3-T_KV"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split and shuffle the dataset into training and validation sets\n",
    "shuffled_dataset = dataset['train'].shuffle(seed=42)  # Set a seed for reproducibility\n",
    "train_valid_split = shuffled_dataset.train_test_split(test_size=0.005) # 99.5% train 0.5% validation\n",
    "train_dataset = train_valid_split['train']\n",
    "eval_dataset = train_valid_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f80fb16a3f5b43aabe0725d94d1835a7",
      "c416e4dbe0a941a0ad2a638c08a9db9b",
      "832aeae92db041ce98cdade3fdbc3ca5",
      "6b067a380e394b46b5bf359637249167",
      "cf23d21a42604b6baa02b3ee4e9d8ba8",
      "e955111318ef435b8bcd765df820aede",
      "7e65dbcf7623429790e6cf94e2199f7e",
      "c6534aa290bf4297852b36ce569d9047",
      "70c4cdb2cb70487f87945e7550fca939",
      "50d0ef9018064a239716376dd361954a",
      "3d5d16950f28469eb30deddd6dce872b",
      "60237c371ac44b73812c40c08e2afbc9",
      "abfe1c2511c04051b4aa46b75ad0f826",
      "e991400ff99e49daa8cd237c84b70418",
      "da546634b39246a3a9890c8c40112594",
      "921fd52182b047fdbf9170c74d432c64",
      "c11f4e4156ad42099af4b267ec48952b",
      "877e767c7e394792a5f834f6d065b9b3",
      "841bcc9f78264780b7c67be4a45a28e3",
      "59277f36aaf54a719d64b22d975c0d51",
      "e08cc14e19554903a110b6899e067652",
      "67854fb6252b4a40b2fc32f255d453c0"
     ]
    },
    "id": "fEeHyA68-puB",
    "outputId": "d384f64a-9cc0-4672-bae3-5452eb06833e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80fb16a3f5b43aabe0725d94d1835a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/995000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60237c371ac44b73812c40c08e2afbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the training dataset and generate prompt for each datapoint\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "JKBG7s8woNuo",
    "outputId": "5b979395-7c01-4c56-b92a-663e4bcaef75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Review the provided math question, its solution, and the final answer. Verify the accuracy of the solution steps and the correctness of the final answer based on the calculations provided.\\nPlease analyze the information below and explicitly state 'True' if the solution accurately solves the problem and the answer matches, or 'False' if it does not. Do not provide additional information or explanation in your response.\\n\\n### Math Question:\\nPaul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\\n\\n### Provided Answer:\\n252\\n\\n### Detailed Solution:\\nLet's solve this problem using Python code.\\n<llm-code>\\nshirts_cost = 4 * 15.00\\npants_cost = 2 * 40.00\\nsuit_cost = 150.00\\nsweaters_cost = 2 * 30.00\\ntotal_cost_before_20_discount = shirts_cost + pants_cost + suit_cost + sweaters_cost\\ndiscount_percent = 20\\ndiscount_portion = discount_percent / 100\\ntotal_cost_after_20_discount = total_cost_before_20_discount - total_cost_before_20_discount * discount_portion\\nten_percent_off_coupon_amount = total_cost_after_20_discount * 10 / 100\\ntotal_cost_after_all_discounts = total_cost_after_20_discount - ten_percent_off_coupon_amount\\ntotal_cost_after_all_discounts\\n</llm-code>\\n<llm-code-output>\\n252.0\\n</llm-code-output>\\nThus Paul paid \\\\boxed{252} dollars for all of his clothes.\\n\\nAnswer is correct (True/False only):True<|end_of_text|>\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print a smaple training example\n",
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egSQOrCJeM7n"
   },
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "49265a0b1ca64ffabe5e1f6865ba52ce",
      "8dda04bc618d44e3bde4adc5e15dbc4c",
      "03bc2eb808264eed9c24aa69880f6583",
      "d9bbc36b567d4ff7bbf744731f69eb6d",
      "b09cf044ee114c9a82f7848d415a41ce",
      "ccbd3d07ef3f4be190068f1ccb4b7399",
      "17805ab26379409da4cfabed3961efaa",
      "b597fef08038422e84d1eb26b80c14f6",
      "0dffe34fa5dd487c9cf7686e9901194f",
      "30ccb8158d1e4314af6fad4d4d7edd67",
      "736b61e45d4747e8883616410223b8a3",
      "0fe208b4fb9c4f2389b68db96802ceb5",
      "24e98f6fda44492bb2fa373c0de9151b",
      "42c800dd68384e68995d19195f8bfb21",
      "b19c84d6f57f4097931ff19e51dc1e2b",
      "30cbcd2a0d254e05b138fd4adc923cec",
      "2615f413d635402b9a43327c481ed939",
      "8d500832a86846018ef5eed388a7fe8d",
      "744e5dde919e49a4a7f1fa17429b9f01",
      "bcb53483642c4980ba3fdec33df86118",
      "894a7c1bc8884988b43706ee61a90e52",
      "76ecac89ca1e4ee58d73e4c4a50785b5"
     ]
    },
    "id": "INoSdVrEbO9Q",
    "outputId": "9b1882f8-214e-4ddb-b075-cd73e640392f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49265a0b1ca64ffabe5e1f6865ba52ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/995000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe208b4fb9c4f2389b68db96802ceb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_steps = 800\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 6,\n",
    "        warmup_steps = int(0.1*max_steps),\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = max_steps,\n",
    "        learning_rate = 4e-4,  # 2e-4, 3e-4, 5e-4\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        # eval_steps=1,  # Evaluate after every logging step; adjust as needed\n",
    "        # eval_strategy=\"steps\",  # Evaluate at each logging step\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_steps=50,            # Save a checkpoint every 10 steps\n",
    "        save_total_limit=3,\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WquBPTm4b-3z",
    "outputId": "1c9dae36-f2ef-4dbc-f851-ba77e0bcbeeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 400\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 3:46:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>0.660618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.638127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.637876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.658276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.730900</td>\n",
       "      <td>0.690449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>0.694319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.689212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.688648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.684463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.687285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.687766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.686067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.629100</td>\n",
       "      <td>0.684718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.678512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.674617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.673919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.669782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.664234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.737300</td>\n",
       "      <td>0.656764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.650614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.645086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.639886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.634936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.631666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.539100</td>\n",
       "      <td>0.626244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.683800</td>\n",
       "      <td>0.620055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.615175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.610409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.587300</td>\n",
       "      <td>0.605431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.557600</td>\n",
       "      <td>0.600470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.619100</td>\n",
       "      <td>0.595721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.571500</td>\n",
       "      <td>0.591804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.548200</td>\n",
       "      <td>0.587754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.606500</td>\n",
       "      <td>0.582446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.578650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.574821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.571803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.571600</td>\n",
       "      <td>0.569363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.568296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlhUYLKS3xd5",
    "outputId": "9cd51366-1694-4d77-b6da-88d9d15c7b08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.6610693509876728, metrics={'train_runtime': 13583.4139, 'train_samples_per_second': 0.707, 'train_steps_per_second': 0.029, 'total_flos': 2.108350115734487e+17, 'train_loss': 0.6610693509876728, 'epoch': 0.009648241206030151})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AgjSAa1glaB"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlK10gqQgnL1"
   },
   "outputs": [],
   "source": [
    "# Sample inferene data point\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "sample_ques = test_dataset['question'][0]\n",
    "sample_ans = test_dataset['answer'][0]\n",
    "sample_sol = test_dataset['solution'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVRq3CmNguyB",
    "outputId": "600ca876-4374-406f-d17c-736f2310ca93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Promt:\n",
      " Review the provided math question, its solution, and the final answer. Verify the accuracy of the solution steps and the correctness of the final answer based on the calculations provided.\n",
      "Please analyze the information below and explicitly state 'True' if the solution accurately solves the problem and the answer matches, or 'False' if it does not. Do not provide additional information or explanation in your response.\n",
      "\n",
      "### Math Question:\n",
      "The Parker family needs to leave the house by 5 pm for a dinner party. Mrs. Parker was waiting to get into the bathroom at 2:30 pm. Her oldest daughter used the bathroom for 45 minutes and her youngest daughter used the bathroom for another 30 minutes. Then her husband used it for 20 minutes. How much time will Mrs. Parker have to use the bathroom to leave on time?\n",
      "\n",
      "### Provided Answer:\n",
      "205\n",
      "\n",
      "### Detailed Solution:\n",
      "Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "minutes_per_hour = 60\n",
      "minutes_left_before_5 = 5 * minutes_per_hour\n",
      "total_time_spent_by_family = 45 + 30 + 20\n",
      "minutes_before_5_after_family = minutes_left_before_5 - total_time_spent_by_family\n",
      "minutes_before_5_after_family\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "205\n",
      "</llm-code-output>\n",
      "Thus Mrs. Parker will have \\boxed{205} minutes in the bathroom before the family leaves.\n",
      "\n",
      "Answer is correct (True/False only):\n",
      "Response: ['False']\n"
     ]
    }
   ],
   "source": [
    "# Running inference on single test\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "input_prompt = prompt.format(\n",
    "        sample_ques, # ques\n",
    "        sample_ans, # given answer\n",
    "        sample_sol, # solution\n",
    "        \"\", # output - leave this blank for generation! LLM willl generate is it is True or False\n",
    "    )\n",
    "\n",
    "print(\"Input Promt:\\n\", input_prompt)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    input_prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "input_shape = inputs['input_ids'].shape\n",
    "input_token_len = input_shape[1] # 1 because of batch\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "# you can get the whole generated text by uncommenting the below line\n",
    "# text_generated = tokenizer.batch_decode([outputs, skip_special_tokens=True)\n",
    "\n",
    "response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmalqrchWeYI"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnbJ7P7RdrII",
    "outputId": "50beedae-0385-4cad-f59c-8a1682d457b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing CUDA memory...\n",
      "CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "def clear_cuda():\n",
    "    torch.cuda.empty_cache()  # Empties the cache\n",
    "    if torch.cuda.is_available():\n",
    "        # Wait for all CUDA kernels to complete\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Resets memory allocator which can free up a significant amount of memory\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "        # Explicitly deletes any tensors on the default device (usually GPU:0) by collecting them\n",
    "        with torch.cuda.device('cuda'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Clearing CUDA memory...\")\n",
    "    clear_cuda()\n",
    "    print(\"CUDA memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FghV7gTBhclr",
    "outputId": "0298ef63-eec7-4f82-bc0a-a9a8ec03e4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOJv4F9Br0ik",
    "outputId": "bcc08de4-b2c5-49fc-b33c-dded224d8535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 23:09:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0              46W / 400W |   7149MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYYoVmGhF-0p",
    "outputId": "7f3bf65d-a6f9-4fa2-b42b-4e3bc8d4d9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0  to  30\n",
      "Batch:  30  to  60\n",
      "Batch:  60  to  90\n",
      "Batch:  90  to  120\n",
      "Batch:  120  to  150\n",
      "Batch:  150  to  180\n",
      "Batch:  180  to  210\n",
      "Batch:  210  to  240\n",
      "Batch:  240  to  270\n",
      "Batch:  270  to  300\n",
      "Batch:  300  to  330\n",
      "Batch:  330  to  360\n",
      "Batch:  360  to  390\n",
      "Batch:  390  to  420\n",
      "Batch:  420  to  450\n",
      "Batch:  450  to  480\n",
      "Batch:  480  to  510\n",
      "Batch:  510  to  540\n",
      "Batch:  540  to  570\n",
      "Batch:  570  to  600\n",
      "Batch:  600  to  630\n",
      "Batch:  630  to  660\n",
      "Batch:  660  to  690\n",
      "Batch:  690  to  720\n",
      "Batch:  720  to  750\n",
      "Batch:  750  to  780\n",
      "Batch:  780  to  810\n",
      "Batch:  810  to  840\n",
      "Batch:  840  to  870\n",
      "Batch:  870  to  900\n",
      "Batch:  900  to  930\n",
      "Batch:  930  to  960\n",
      "Batch:  960  to  990\n",
      "Batch:  990  to  1020\n",
      "Batch:  1020  to  1050\n",
      "Batch:  1050  to  1080\n",
      "Batch:  1080  to  1110\n",
      "Batch:  1110  to  1140\n",
      "Batch:  1140  to  1170\n",
      "Batch:  1170  to  1200\n",
      "Batch:  1200  to  1230\n",
      "Batch:  1230  to  1260\n",
      "Batch:  1260  to  1290\n",
      "Batch:  1290  to  1320\n",
      "Batch:  1320  to  1350\n",
      "Batch:  1350  to  1380\n",
      "Batch:  1380  to  1410\n",
      "Batch:  1410  to  1440\n",
      "Batch:  1440  to  1470\n",
      "Batch:  1470  to  1500\n",
      "Batch:  1500  to  1530\n",
      "Batch:  1530  to  1560\n",
      "Batch:  1560  to  1590\n",
      "Batch:  1590  to  1620\n",
      "Batch:  1620  to  1650\n",
      "Batch:  1650  to  1680\n",
      "Batch:  1680  to  1710\n",
      "Batch:  1710  to  1740\n",
      "Batch:  1740  to  1770\n",
      "Batch:  1770  to  1800\n",
      "Batch:  1800  to  1830\n",
      "Batch:  1830  to  1860\n",
      "Batch:  1860  to  1890\n",
      "Batch:  1890  to  1920\n",
      "Batch:  1920  to  1950\n",
      "Batch:  1950  to  1980\n",
      "Batch:  1980  to  2010\n",
      "Batch:  2010  to  2040\n",
      "Batch:  2040  to  2070\n",
      "Batch:  2070  to  2100\n",
      "Batch:  2100  to  2130\n",
      "Batch:  2130  to  2160\n",
      "Batch:  2160  to  2190\n",
      "Batch:  2190  to  2220\n",
      "Batch:  2220  to  2250\n",
      "Batch:  2250  to  2280\n",
      "Batch:  2280  to  2310\n",
      "Batch:  2310  to  2340\n",
      "Batch:  2340  to  2370\n",
      "Batch:  2370  to  2400\n",
      "Batch:  2400  to  2430\n",
      "Batch:  2430  to  2460\n",
      "Batch:  2460  to  2490\n",
      "Batch:  2490  to  2520\n",
      "Batch:  2520  to  2550\n",
      "Batch:  2550  to  2580\n",
      "Batch:  2580  to  2610\n",
      "Batch:  2610  to  2640\n",
      "Batch:  2640  to  2670\n",
      "Batch:  2670  to  2700\n",
      "Batch:  2700  to  2730\n",
      "Batch:  2730  to  2760\n",
      "Batch:  2760  to  2790\n",
      "Batch:  2790  to  2820\n",
      "Batch:  2820  to  2850\n",
      "Batch:  2850  to  2880\n",
      "Batch:  2880  to  2910\n",
      "Batch:  2910  to  2940\n",
      "Batch:  2940  to  2970\n",
      "Batch:  2970  to  3000\n",
      "Batch:  3000  to  3030\n",
      "Batch:  3030  to  3060\n",
      "Batch:  3060  to  3090\n",
      "Batch:  3090  to  3120\n",
      "Batch:  3120  to  3150\n",
      "Batch:  3150  to  3180\n",
      "Batch:  3180  to  3210\n",
      "Batch:  3210  to  3240\n",
      "Batch:  3240  to  3270\n",
      "Batch:  3270  to  3300\n",
      "Batch:  3300  to  3330\n",
      "Batch:  3330  to  3360\n",
      "Batch:  3360  to  3390\n",
      "Batch:  3390  to  3420\n",
      "Batch:  3420  to  3450\n",
      "Batch:  3450  to  3480\n",
      "Batch:  3480  to  3510\n",
      "Batch:  3510  to  3540\n",
      "Batch:  3540  to  3570\n",
      "Batch:  3570  to  3600\n",
      "Batch:  3600  to  3630\n",
      "Batch:  3630  to  3660\n",
      "Batch:  3660  to  3690\n",
      "Batch:  3690  to  3720\n",
      "Batch:  3720  to  3750\n",
      "Batch:  3750  to  3780\n",
      "Batch:  3780  to  3810\n",
      "Batch:  3810  to  3840\n",
      "Batch:  3840  to  3870\n",
      "Batch:  3870  to  3900\n",
      "Batch:  3900  to  3930\n",
      "Batch:  3930  to  3960\n",
      "Batch:  3960  to  3990\n",
      "Batch:  3990  to  4020\n",
      "Batch:  4020  to  4050\n",
      "Batch:  4050  to  4080\n",
      "Batch:  4080  to  4110\n",
      "Batch:  4110  to  4140\n",
      "Batch:  4140  to  4170\n",
      "Batch:  4170  to  4200\n",
      "Batch:  4200  to  4230\n",
      "Batch:  4230  to  4260\n",
      "Batch:  4260  to  4290\n",
      "Batch:  4290  to  4320\n",
      "Batch:  4320  to  4350\n",
      "Batch:  4350  to  4380\n",
      "Batch:  4380  to  4410\n",
      "Batch:  4410  to  4440\n",
      "Batch:  4440  to  4470\n",
      "Batch:  4470  to  4500\n",
      "Batch:  4500  to  4530\n",
      "Batch:  4530  to  4560\n",
      "Batch:  4560  to  4590\n",
      "Batch:  4590  to  4620\n",
      "Batch:  4620  to  4650\n",
      "Batch:  4650  to  4680\n",
      "Batch:  4680  to  4710\n",
      "Batch:  4710  to  4740\n",
      "Batch:  4740  to  4770\n",
      "Batch:  4770  to  4800\n",
      "Batch:  4800  to  4830\n",
      "Batch:  4830  to  4860\n",
      "Batch:  4860  to  4890\n",
      "Batch:  4890  to  4920\n",
      "Batch:  4920  to  4950\n",
      "Batch:  4950  to  4980\n",
      "Batch:  4980  to  5010\n",
      "Collected Responses: ['False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'True', 'True', 'True', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'True', 'False', 'True', 'True', 'False', 'False']\n",
      "Actual Correctness: [False, False, False, False, True, True, False, False, True, True, False, True, True, False, False, True, False, True, True, True, False, True, True, False, True, True, True, False, True, False, True, False, True, True, True, True, True, False, False, False, True, False, False, False, True, False, False, False, True, False, True, False, False, True, False, True, False, False, False, True, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, True, True, False, True, True, True, False, False, False, True, False, True, True, True, False, False, True, True, False, False, False, True, True, False, True, False, False, False, False, True, False, True, True, False, True, False, True, False, True, False, False, True, True, True, False, False, True, True, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, True, True, True, False, True, False, False, True, False, False, False, False, True, False, True, True, False, False, True, True, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, True, True, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, False, True, True, False, True, False, False, True, False, False, False, False, True, True, True, True, False, True, True, True, True, False, False, True, False, False, True, True, True, True, False, True, False, True, True, True, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, True, True, True, False, False, False, True, False, True, False, False, True, True, False, True, False, False, False, False, False, False, False, True, False, True, True, False, False, False, True, False, True, False, False, True, False, True, True, False, False, True, True, False, True, True, True, False, True, False, True, False, True, True, True, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, True, True, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, False, True, False, False, True, True, True, True, False, False, True, True, True, True, True, True, False, False, False, True, False, True, True, False, True, False, False, True, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, False, False, True, True, True, True, False, True, False, False, False, False, True, False, True, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, True, False, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, True, True, False, False, False, False, True, True, True, False, False, False, True, True, False, False, False, False, True, True, False, False, False, False, False, True, True, False, False, True, True, True, False, True, False, True, False, False, False, True, True, True, True, True, False, True, False, True, False, True, False, False, True, False, False, False, True, True, False, False, False, False, True, True, False, True, True, False, True, False, True, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, True, True, True, False, False, True, False, False, False, False, False, True, False, True, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, False, False, False, True, False, False, False, False, False, False, False, True, False, True, True, True, False, False, False, False, False, False, True, False, True, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, True, False, True, True, False, False, True, True, False, False, True, False, True, True, False, False, False, True, True, False, False, False, True, False, False, False, False, False, True, False, False, False, True, True, False, True, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, False, False, False, True, False, False, True, True, True, False, False, False, True, True, True, False, True, True, False, True, False, False, False, False, True, True, False, False, False, False, True, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, True, False, True, False, True, True, True, False, True, False, False, True, True, False, False, False, False, True, False, True, True, False, False, False, False, False, False, False, True, False, True, False, False, False, True, False, True, False, False, True, False, True, False, True, False, False, False, True, False, True, False, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, True, True, False, True, False, True, False, False, True, True, False, False, False, False, False, False, True, False, True, True, True, False, False, False, True, False, True, True, True, False, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, True, False, True, False, False, False, True, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, True, False, False, True, True, False, False, True, False, True, True, False, False, False, True, False, False, False, False, True, False, True, False, True, True, True, True, True, True, False, False, False, True, True, False, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, False, False, False, False, True, False, True, True, False, False, True, True, True, False, True, False, True, True, False, False, False, True, False, False, True, False, True, False, True, True, True, True, False, False, False, True, False, True, True, False, True, False, False, True, False, False, True, False, False, False, True, False, False, True, True, True, False, False, True, True, False, False, True, False, True, False, False, True, True, True, False, False, True, False, False, False, False, True, False, False, True, False, False, True, True, False, False, False, True, False, True, False, False, True, True, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, True, True, True, False, False, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, False, True, True, True, True, False, False, False, False, True, False, True, False, True, False, False, False, False, False, True, False, True, False, True, True, False, False, True, False, False, False, False, True, True, True, False, True, False, False, True, True, True, False, False, True, True, True, True, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, False, True, False, False, False, True, False, True, False, False, True, False, False, True, False, True, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, True, True, True, True, False, False, False, True, True, True, False, False, False, True, False, True, False, True, False, True, True, False, False, True, False, True, False, False, False, False, False, True, False, False, True, False, False, False, False, False, True, False, False, False, False, True, True, False, False, True, False, True, False, False, False, True, False, False, False, True, True, True, True, False, False, False, True, False, False, False, True, False, False, False, True, True, True, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, True, False, False, True, False, False, False, False, True, False, True, False, False, True, True, False, False, True, True, True, True, False, False, False, True, True, True, True, True, False, True, False, True, False, False, False, False, False, True, True, False, False, True, False, False, True, False, False, True, False, True, True, False, False, True, False, False, False, True, True, False, False, True, False, False, True, True, True, False, False, True, False, True, True, True, True, False, False, False, True, False, True, True, True, False, True, True, True, False, True, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, False, True, True, False, False, False, True, True, False, True, False, False, False, False, True, True, True, False, True, False, False, False, True, True, False, True, True, False, False, False, True, False, True, True, False, True, False, False, True, True, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, True, True, False, False, True, False, False, False, True, True, False, False, True, False, False, True, False, False, True, True, False, False, False, True, True, False, False, False, False, True, False, True, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, True, True, False, False, False, False, True, False, False, True, True, False, True, False, True, True, False, False, True, True, True, True, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, False, False, True, False, False, False, True, False, False, True, False, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, False, False, False, True, True, False, False, False, False, True, False, False, True, False, False, True, False, True, False, True, False, False, False, False, True, False, False, False, False, True, False, True, False, True, False, False, True, False, False, True, False, False, False, False, False, False, True, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, True, False, True, False, False, False, False, False, True, True, False, True, False, True, True, False, False, False, True, True, True, False, False, True, False, True, False, False, True, True, False, False, False, False, True, False, False, False, False, False, True, False, False, True, True, False, False, True, False, True, True, True, False, True, True, False, False, False, False, True, False, True, False, True, True, False, False, True, False, False, False, True, False, True, False, False, True, False, True, True, True, True, False, False, False, True, False, True, False, True, False, False, False, False, False, False, True, True, True, True, False, False, False, True, False, False, True, True, False, True, False, False, True, True, False, True, True, True, False, False, True, True, True, False, False, False, False, False, True, False, True, False, False, True, True, False, False, False, True, True, True, True, False, True, False, True, True, False, False, True, False, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, True, True, True, False, True, False, True, False, False, False, False, True, False, True, False, True, False, False, True, True, False, False, True, False, False, True, False, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, False, False, True, True, False, True, False, True, False, False, False, False, True, True, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, False, False, False, False, False, False, True, False, True, True, False, False, True, False, False, True, True, True, False, False, False, False, False, False, False, True, True, False, False, False, False, True, False, True, False, True, False, False, False, True, True, True, True, False, False, True, False, False, False, True, True, False, True, False, True, False, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, True, False, False, True, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, True, False, False, True, True, False, True, False, False, False, False, True, True, True, False, True, False, True, True, False, False, False, True, False, True, True, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, True, False, True, True, False, False, True, True, True, False, False, True, True, False, True, False, False, True, False, True, True, False, False, True, False, False, False, False, False, True, True, False, True, False, False, False, True, True, False, True, False, True, True, False, True, False, True, False, True, False, False, True, True, False, True, False, True, False, True, False, False, True, False, True, True, False, True, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, True, True, False, False, False, True, False, False, True, False, True, False, False, True, False, False, False, True, False, False, True, True, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, True, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, True, True, False, True, False, True, True, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, True, False, True, True, True, False, True, True, False, False, False, True, False, False, False, False, True, False, False, False, False, True, True, False, True, False, False, False, True, True, False, True, False, False, False, True, True, True, False, False, True, False, False, False, True, True, True, True, False, True, False, False, False, True, True, False, False, True, False, False, True, True, True, True, False, False, True, True, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, True, False, True, False, True, True, False, False, False, False, True, True, False, False, True, False, True, True, False, True, True, True, True, True, False, False, False, False, True, False, True, True, False, False, False, True, False, True, True, True, True, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, False, True, True, False, False, False, True, True, True, False, True, False, False, False, True, False, False, True, False, True, True, False, False, False, True, True, False, False, False, False, True, False, False, True, False, False, True, False, True, True, True, False, True, False, False, True, True, False, True, True, True, False, True, False, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, True, False, False, True, True, True, True, True, True, False, True, True, True, False, False, False, False, False, False, False, False, False, True, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, False, True, True, False, True, True, True, True, True, False, False, True, False, False, False, True, True, True, False, False, True, False, False, False, False, True, False, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, False, False, True, False, False, False, True, False, True, False, True, True, False, True, False, False, False, False, False, False, False, True, True, True, False, True, True, False, False, True, True, False, True, True, True, False, False, True, False, False, True, False, True, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, False, True, True, True, True, True, True, False, False, True, True, False, False, True, False, False, True, True, True, False, False, False, False, True, False, True, False, True, False, False, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, True, True, True, True, True, False, True, False, True, True, False, True, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, True, True, False, False, True, False, True, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, True, True, True, False, True, True, False, True, False, False, False, True, True, True, True, False, False, False, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, True, False, False, True, False, False, True, True, False, True, False, True, False, False, False, True, False, True, True, True, False, False, False, True, True, True, False, False, True, False, False, False, True, False, False, True, False, True, False, True, False, True, False, False, True, False, False, False, False, True, False, True, False, True, False, False, False, False, True, True, False, False, True, False, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, True, False, False, True, False, True, True, False, True, False, False, False, True, True, False, True, False, False, False, False, False, True, False, False, False, True, True, True, False, False, True, False, False, False, False, True, False, True, True, True, False, False, True, False, True, False, False, True, False, False, True, False, False, True, True, True, True, True, True, False, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, True, True, False, False, False, True, False, True, True, True, True, False, False, True, True, False, False, True, False, True, False, False, True, True, True, False, False, False, True, False, False, False, True, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, True, True, True, False, False, False, True, False, False, False, False, False, True, False, True, True, False, False, False, False, False, True, False, True, False, False, True, False, True, False, False, False, False, True, True, False, True, False, True, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, False, True, False, False, True, False, True, True, True, False, True, False, True, False, True, False, False, True, False, False, True, True, True, True, False, True, False, True, False, True, False, False, True, False, False, False, True, False, False, False, True, True, False, False, True, True, True, False, False, True, False, False, False, True, True, True, False, True, False, False, False, False, True, True, True, True, False, False, True, False, True, False, False, True, False, True, False, False, False, False, True, True, False, False, False, True, False, False, True, False, True, False, False, False, False, False, True, True, True, False, False, True, True, False, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, True, False, True, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, True, False, False, False, False, True, False, True, False, False, False, False, True, False, True, True, False, False, False, False, True, True, False, False, True, False, False, True, False, True, True, False, True, True, False, True, True, False, True, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, False, True, True, False, False, True, False, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, True, True, False, True, True, True, False, False, False, False, True, True, True, False, False, False, False, False, True, False, False, False, False, True, True, True, False, False, False, False, False, False, False, True, True, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, True, True, True, True, True, True, False, False, False, False, False, False, True, True, True, True, False, True, True, False, False, False, True, False, False, False, True, True, False, True, True, False, False, True, False, False, False, False, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, False, True, False, True, True, False, False, False, True, True, True, False, False, True, False, True, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, False, True, True, True, False, False, True, False, False, True, False, True, False, True, False, False, True, False, True, False, False, False, False, True, False, False, True, False, True, True, True, False, True, False, False, False, True, False, False, False, True, False, False, True, True, False, False, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, False, False, False, False, True, True, False, False, False, True, False, False, True, True, True, False, False, False, False, False, True, True, False, True, True, True, False, False, False, True, False, False, True, True, True, False, False, True, False, True, False, True, False, True, False, False, False, False, True, True, True, False, True, False, False, True, False, True, False, True, True, False, False, False, False, True, True, True, False, False, False, True, True, True, False, False, True, False, False, False, True, False, True, False, True, True, False, True, False, False, True, False, True, True, True, True, True, True, True, True, False, False, False, False, True, False, True, False, True, True, False, False, False, True, False, False, False, True, False, True, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, True, False, False, False, False, True, True, True, True, False, False, False, False, False, False, False, True, True, False, False, True, False, True, False, True, True, False, False, False, True, True, False, False, False, False, True, False, False, False, True, True, False, False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, True, True, False, False, False, True, False, False, True, False, True, True, False, True, True, True, True, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, True, True, False, False, False, False, True, False, True, False, False, True, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, True, True, True, True, False, False, True, True, False, False, False, False, False, False, True, True, True, True, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, True, True, False, False, True, True, False, False, False, True, True, False, True, False, True, False, False, False, False, False, True, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, False, False, False, True, False, False, True, True, False, False, False, True, False, False, False, True, False, False, True, False, False, False, False, False, True, True, True, False, False, True, False, False, False, True, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, True, False, True, False, True, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, True, False, False, True, True, True, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, True, True, True, False, False, False, False, True, True, False, False, True, False, True, False, True, True, False, True, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, False, True, True, False, False, False, False, True, False, True, False, True, False, False, True, False, False, True, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, True, True, True, False, False, False, True, False, False, False, False, False, True, True, True, False, False, True, False, False, False, True, True, False, True, True, False, True, False, False, False, True, True, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, False, False, True, True, False, True, True, False, False, False, False, False, True, True, True, False, True, False, True, True, True, False, True, True, False, False, True, False, False, False, False, True, True, False, False, False, True, True, False, True, False, True, False, False, False, True, False, False, True, False, False, True, True, True, True, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, True, False, True, False, True, True, False, False, False, False, False, True, False, False, False, False, True, False, True, False, True, False, True, True, False, False, False, True, True, False, False, True, False, False, False, True, False, True, True, True, False, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, True, True, False, True, True, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, True, False, False, False, False, True, False, False, True, True, False, False, False, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, True, True, True, True, True, False, True, True, False, False, True, False, True, False, False, False, True, True, True, False, False, False, True, False, True, True, False, False, False, True, False, False, False, True, False, False, True, False, True, False, True, True, True, True, False, False, False, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, True, False, True, False, False, False, True, False, True, False, True, False, False, True, False, False, False, False, False, False, False, True, True, False, False, True, True, False, False, True, False, False, False, True, False, True, False, False, False, True, False, False, True, True, False, True, True, True, True, False, False, True, False, False, True, True, False, False, True, False, False, True, False, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False, False, False, False, False, True, False, False, False, False, True, False, True, True, False, False, False, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, True, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, True, True, True, False, False, True, False, False, False, False, True, False, True, True, True, False, False, False, True, True, False, False, False, False, True, False, True, True, False, False, False, False, True, True, False, True, False, True, False, True, False, True, False, False, False, False, False, True, True, False, False, False, False, True, True, True, False, True, True, False, True, False, False, True, False, True, True, False, True, True, True, False, True, False, True, False, True, True, True, True, False, True, False, False, False, True, False, True, False, True, True, True, False, False, False, False, False, True, False, True, True, False, False, False, False, True, True, False, True, False, True, True, True, True, False, False, True, True, True, False, True, False, True, False, False, True, False, True, True, True, False, False, True, False, False, False, False, True, True, False, True, True, False, False, False, False, False, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, False, True, True, False, True, False, True, True, True, False, False, False, False, False, True, True, False, False, False, False, False, True, False, True, True, False, True, False, False, True, True, True, True, True, False, True, False, False, False, True, False, True, False, False, True, False, True, False, False, False, False, True, False, True, True, False, False, False, True, False, False, False, False, True, False, True, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, True, True, True, False, False, False, True, True, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, True, True, False, False]\n",
      "Comparison Results: [True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, False, True, True, True, False, False, True, True, True, True, True, True, False, True, False, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, False, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, False, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, False, False, True, True, False, True, True, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, False, False, True, False, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, False, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, False, False, False, True, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, False, True, True, False, True, False, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, False, True, True, False, False, True, False, True, False, True, True, False, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, False, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, False, True, True, False, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, False, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, False, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, True, True, True, False, True, False, True, True, True, True, False, True, False, True, True, True, False, False, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, False, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, False, False, True, False, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, False, False, False, True, False, True, True, False, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True]\n",
      "Model Accuracy: 85.10%\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "batch_size = 30\n",
    "responses = []  # List to store all responses\n",
    "actual_correctness = []  # List to store actual correctness\n",
    "comparison_results = []  # List to store comparison results\n",
    "\n",
    "# Process the dataset in batches\n",
    "for i in range(0, len(eval_dataset['question']), batch_size):\n",
    "    print(\"Batch: \", i,\" to \", i+batch_size)\n",
    "    batch_questions = eval_dataset['question'][i:i+batch_size]\n",
    "    batch_answers = eval_dataset['answer'][i:i+batch_size]\n",
    "    batch_solutions = eval_dataset['solution'][i:i+batch_size]\n",
    "    batch_is_correct = eval_dataset['is_correct'][i:i+batch_size]  # Actual correctness\n",
    "\n",
    "    # Generate input prompts for the batch\n",
    "    input_prompts = [\n",
    "        prompt.format(q, a, s, \"\")\n",
    "        for q, a, s in zip(batch_questions, batch_answers, batch_solutions)\n",
    "    ]\n",
    "\n",
    "    # Tokenize inputs as a batch\n",
    "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    input_token_lens = inputs['input_ids'].shape[1]  # Total length of the input tokens\n",
    "\n",
    "    # Generate outputs for the entire batch\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "    batch_responses = [\n",
    "        tokenizer.decode(outputs[j][input_token_lens:], skip_special_tokens=True).strip()\n",
    "        for j in range(outputs.size(0))\n",
    "    ]\n",
    "    responses.extend(batch_responses)  # Append batch responses to the main list\n",
    "\n",
    "    # Collect actual correctness and compare\n",
    "    actual_correctness.extend(batch_is_correct)\n",
    "    batch_comparison = [str(resp).lower() == str(ac).strip().lower() for resp, ac in zip(batch_responses, batch_is_correct)]\n",
    "    # print(\"Batch Comparison: \", batch_comparison)\n",
    "    comparison_results.extend(batch_comparison)  # Append comparison results\n",
    "    # print(\"Comparison Results: \", comparison_results[i:])\n",
    "\n",
    "\n",
    "    # Clean up GPU memory after processing each batch\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    clear_cuda()\n",
    "\n",
    "# After processing all batches\n",
    "print(\"Collected Responses:\", responses)\n",
    "print(\"Actual Correctness:\", actual_correctness)\n",
    "print(\"Comparison Results:\", comparison_results)\n",
    "\n",
    "# # Optionally, analyze the comparison results\n",
    "accuracy = sum(comparison_results) / len(comparison_results)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MacqrorOhEHS"
   },
   "source": [
    "## Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBinNoZ8nnUK"
   },
   "outputs": [],
   "source": [
    "clear_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXUN6EkxnorF",
    "outputId": "142f73dc-39e5-45d6-9437-859d3e7ef2ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkYyL89RX5PO",
    "outputId": "5e2cf277-1ee1-42de-ecac-e1590c3cba4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0  to  30\n",
      "Batch:  30  to  60\n",
      "Batch:  60  to  90\n",
      "Batch:  90  to  120\n",
      "Batch:  120  to  150\n",
      "Batch:  150  to  180\n",
      "Batch:  180  to  210\n",
      "Batch:  210  to  240\n",
      "Batch:  240  to  270\n",
      "Batch:  270  to  300\n",
      "Batch:  300  to  330\n",
      "Batch:  330  to  360\n",
      "Batch:  360  to  390\n",
      "Batch:  390  to  420\n",
      "Batch:  420  to  450\n",
      "Batch:  450  to  480\n",
      "Batch:  480  to  510\n",
      "Batch:  510  to  540\n",
      "Batch:  540  to  570\n",
      "Batch:  570  to  600\n",
      "Batch:  600  to  630\n",
      "Batch:  630  to  660\n",
      "Batch:  660  to  690\n",
      "Batch:  690  to  720\n",
      "Batch:  720  to  750\n",
      "Batch:  750  to  780\n",
      "Batch:  780  to  810\n",
      "Batch:  810  to  840\n",
      "Batch:  840  to  870\n",
      "Batch:  870  to  900\n",
      "Batch:  900  to  930\n",
      "Batch:  930  to  960\n",
      "Batch:  960  to  990\n",
      "Batch:  990  to  1020\n",
      "Batch:  1020  to  1050\n",
      "Batch:  1050  to  1080\n",
      "Batch:  1080  to  1110\n",
      "Batch:  1110  to  1140\n",
      "Batch:  1140  to  1170\n",
      "Batch:  1170  to  1200\n",
      "Batch:  1200  to  1230\n",
      "Batch:  1230  to  1260\n",
      "Batch:  1260  to  1290\n",
      "Batch:  1290  to  1320\n",
      "Batch:  1320  to  1350\n",
      "Batch:  1350  to  1380\n",
      "Batch:  1380  to  1410\n",
      "Batch:  1410  to  1440\n",
      "Batch:  1440  to  1470\n",
      "Batch:  1470  to  1500\n",
      "Batch:  1500  to  1530\n",
      "Batch:  1530  to  1560\n",
      "Batch:  1560  to  1590\n",
      "Batch:  1590  to  1620\n",
      "Batch:  1620  to  1650\n",
      "Batch:  1650  to  1680\n",
      "Batch:  1680  to  1710\n",
      "Batch:  1710  to  1740\n",
      "Batch:  1740  to  1770\n",
      "Batch:  1770  to  1800\n",
      "Batch:  1800  to  1830\n",
      "Batch:  1830  to  1860\n",
      "Batch:  1860  to  1890\n",
      "Batch:  1890  to  1920\n",
      "Batch:  1920  to  1950\n",
      "Batch:  1950  to  1980\n",
      "Batch:  1980  to  2010\n",
      "Batch:  2010  to  2040\n",
      "Batch:  2040  to  2070\n",
      "Batch:  2070  to  2100\n",
      "Batch:  2100  to  2130\n",
      "Batch:  2130  to  2160\n",
      "Batch:  2160  to  2190\n",
      "Batch:  2190  to  2220\n",
      "Batch:  2220  to  2250\n",
      "Batch:  2250  to  2280\n",
      "Batch:  2280  to  2310\n",
      "Batch:  2310  to  2340\n",
      "Batch:  2340  to  2370\n",
      "Batch:  2370  to  2400\n",
      "Batch:  2400  to  2430\n",
      "Batch:  2430  to  2460\n",
      "Batch:  2460  to  2490\n",
      "Batch:  2490  to  2520\n",
      "Batch:  2520  to  2550\n",
      "Batch:  2550  to  2580\n",
      "Batch:  2580  to  2610\n",
      "Batch:  2610  to  2640\n",
      "Batch:  2640  to  2670\n",
      "Batch:  2670  to  2700\n",
      "Batch:  2700  to  2730\n",
      "Batch:  2730  to  2760\n",
      "Batch:  2760  to  2790\n",
      "Batch:  2790  to  2820\n",
      "Batch:  2820  to  2850\n",
      "Batch:  2850  to  2880\n",
      "Batch:  2880  to  2910\n",
      "Batch:  2910  to  2940\n",
      "Batch:  2940  to  2970\n",
      "Batch:  2970  to  3000\n",
      "Batch:  3000  to  3030\n",
      "Batch:  3030  to  3060\n",
      "Batch:  3060  to  3090\n",
      "Batch:  3090  to  3120\n",
      "Batch:  3120  to  3150\n",
      "Batch:  3150  to  3180\n",
      "Batch:  3180  to  3210\n",
      "Batch:  3210  to  3240\n",
      "Batch:  3240  to  3270\n",
      "Batch:  3270  to  3300\n",
      "Batch:  3300  to  3330\n",
      "Batch:  3330  to  3360\n",
      "Batch:  3360  to  3390\n",
      "Batch:  3390  to  3420\n",
      "Batch:  3420  to  3450\n",
      "Batch:  3450  to  3480\n",
      "Batch:  3480  to  3510\n",
      "Batch:  3510  to  3540\n",
      "Batch:  3540  to  3570\n",
      "Batch:  3570  to  3600\n",
      "Batch:  3600  to  3630\n",
      "Batch:  3630  to  3660\n",
      "Batch:  3660  to  3690\n",
      "Batch:  3690  to  3720\n",
      "Batch:  3720  to  3750\n",
      "Batch:  3750  to  3780\n",
      "Batch:  3780  to  3810\n",
      "Batch:  3810  to  3840\n",
      "Batch:  3840  to  3870\n",
      "Batch:  3870  to  3900\n",
      "Batch:  3900  to  3930\n",
      "Batch:  3930  to  3960\n",
      "Batch:  3960  to  3990\n",
      "Batch:  3990  to  4020\n",
      "Batch:  4020  to  4050\n",
      "Batch:  4050  to  4080\n",
      "Batch:  4080  to  4110\n",
      "Batch:  4110  to  4140\n",
      "Batch:  4140  to  4170\n",
      "Batch:  4170  to  4200\n",
      "Batch:  4200  to  4230\n",
      "Batch:  4230  to  4260\n",
      "Batch:  4260  to  4290\n",
      "Batch:  4290  to  4320\n",
      "Batch:  4320  to  4350\n",
      "Batch:  4350  to  4380\n",
      "Batch:  4380  to  4410\n",
      "Batch:  4410  to  4440\n",
      "Batch:  4440  to  4470\n",
      "Batch:  4470  to  4500\n",
      "Batch:  4500  to  4530\n",
      "Batch:  4530  to  4560\n",
      "Batch:  4560  to  4590\n",
      "Batch:  4590  to  4620\n",
      "Batch:  4620  to  4650\n",
      "Batch:  4650  to  4680\n",
      "Batch:  4680  to  4710\n",
      "Batch:  4710  to  4740\n",
      "Batch:  4740  to  4770\n",
      "Batch:  4770  to  4800\n",
      "Batch:  4800  to  4830\n",
      "Batch:  4830  to  4860\n",
      "Batch:  4860  to  4890\n",
      "Batch:  4890  to  4920\n",
      "Batch:  4920  to  4950\n",
      "Batch:  4950  to  4980\n",
      "Batch:  4980  to  5010\n",
      "Batch:  5010  to  5040\n",
      "Batch:  5040  to  5070\n",
      "Batch:  5070  to  5100\n",
      "Batch:  5100  to  5130\n",
      "Batch:  5130  to  5160\n",
      "Batch:  5160  to  5190\n",
      "Batch:  5190  to  5220\n",
      "Batch:  5220  to  5250\n",
      "Batch:  5250  to  5280\n",
      "Batch:  5280  to  5310\n",
      "Batch:  5310  to  5340\n",
      "Batch:  5340  to  5370\n",
      "Batch:  5370  to  5400\n",
      "Batch:  5400  to  5430\n",
      "Batch:  5430  to  5460\n",
      "Batch:  5460  to  5490\n",
      "Batch:  5490  to  5520\n",
      "Batch:  5520  to  5550\n",
      "Batch:  5550  to  5580\n",
      "Batch:  5580  to  5610\n",
      "Batch:  5610  to  5640\n",
      "Batch:  5640  to  5670\n",
      "Batch:  5670  to  5700\n",
      "Batch:  5700  to  5730\n",
      "Batch:  5730  to  5760\n",
      "Batch:  5760  to  5790\n",
      "Batch:  5790  to  5820\n",
      "Batch:  5820  to  5850\n",
      "Batch:  5850  to  5880\n",
      "Batch:  5880  to  5910\n",
      "Batch:  5910  to  5940\n",
      "Batch:  5940  to  5970\n",
      "Batch:  5970  to  6000\n",
      "Batch:  6000  to  6030\n",
      "Batch:  6030  to  6060\n",
      "Batch:  6060  to  6090\n",
      "Batch:  6090  to  6120\n",
      "Batch:  6120  to  6150\n",
      "Batch:  6150  to  6180\n",
      "Batch:  6180  to  6210\n",
      "Batch:  6210  to  6240\n",
      "Batch:  6240  to  6270\n",
      "Batch:  6270  to  6300\n",
      "Batch:  6300  to  6330\n",
      "Batch:  6330  to  6360\n",
      "Batch:  6360  to  6390\n",
      "Batch:  6390  to  6420\n",
      "Batch:  6420  to  6450\n",
      "Batch:  6450  to  6480\n",
      "Batch:  6480  to  6510\n",
      "Batch:  6510  to  6540\n",
      "Batch:  6540  to  6570\n",
      "Batch:  6570  to  6600\n",
      "Batch:  6600  to  6630\n",
      "Batch:  6630  to  6660\n",
      "Batch:  6660  to  6690\n",
      "Batch:  6690  to  6720\n",
      "Batch:  6720  to  6750\n",
      "Batch:  6750  to  6780\n",
      "Batch:  6780  to  6810\n",
      "Batch:  6810  to  6840\n",
      "Batch:  6840  to  6870\n",
      "Batch:  6870  to  6900\n",
      "Batch:  6900  to  6930\n",
      "Batch:  6930  to  6960\n",
      "Batch:  6960  to  6990\n",
      "Batch:  6990  to  7020\n",
      "Batch:  7020  to  7050\n",
      "Batch:  7050  to  7080\n",
      "Batch:  7080  to  7110\n",
      "Batch:  7110  to  7140\n",
      "Batch:  7140  to  7170\n",
      "Batch:  7170  to  7200\n",
      "Batch:  7200  to  7230\n",
      "Batch:  7230  to  7260\n",
      "Batch:  7260  to  7290\n",
      "Batch:  7290  to  7320\n",
      "Batch:  7320  to  7350\n",
      "Batch:  7350  to  7380\n",
      "Batch:  7380  to  7410\n",
      "Batch:  7410  to  7440\n",
      "Batch:  7440  to  7470\n",
      "Batch:  7470  to  7500\n",
      "Batch:  7500  to  7530\n",
      "Batch:  7530  to  7560\n",
      "Batch:  7560  to  7590\n",
      "Batch:  7590  to  7620\n",
      "Batch:  7620  to  7650\n",
      "Batch:  7650  to  7680\n",
      "Batch:  7680  to  7710\n",
      "Batch:  7710  to  7740\n",
      "Batch:  7740  to  7770\n",
      "Batch:  7770  to  7800\n",
      "Batch:  7800  to  7830\n",
      "Batch:  7830  to  7860\n",
      "Batch:  7860  to  7890\n",
      "Batch:  7890  to  7920\n",
      "Batch:  7920  to  7950\n",
      "Batch:  7950  to  7980\n",
      "Batch:  7980  to  8010\n",
      "Batch:  8010  to  8040\n",
      "Batch:  8040  to  8070\n",
      "Batch:  8070  to  8100\n",
      "Batch:  8100  to  8130\n",
      "Batch:  8130  to  8160\n",
      "Batch:  8160  to  8190\n",
      "Batch:  8190  to  8220\n",
      "Batch:  8220  to  8250\n",
      "Batch:  8250  to  8280\n",
      "Batch:  8280  to  8310\n",
      "Batch:  8310  to  8340\n",
      "Batch:  8340  to  8370\n",
      "Batch:  8370  to  8400\n",
      "Batch:  8400  to  8430\n",
      "Batch:  8430  to  8460\n",
      "Batch:  8460  to  8490\n",
      "Batch:  8490  to  8520\n",
      "Batch:  8520  to  8550\n",
      "Batch:  8550  to  8580\n",
      "Batch:  8580  to  8610\n",
      "Batch:  8610  to  8640\n",
      "Batch:  8640  to  8670\n",
      "Batch:  8670  to  8700\n",
      "Batch:  8700  to  8730\n",
      "Batch:  8730  to  8760\n",
      "Batch:  8760  to  8790\n",
      "Batch:  8790  to  8820\n",
      "Batch:  8820  to  8850\n",
      "Batch:  8850  to  8880\n",
      "Batch:  8880  to  8910\n",
      "Batch:  8910  to  8940\n",
      "Batch:  8940  to  8970\n",
      "Batch:  8970  to  9000\n",
      "Batch:  9000  to  9030\n",
      "Batch:  9030  to  9060\n",
      "Batch:  9060  to  9090\n",
      "Batch:  9090  to  9120\n",
      "Batch:  9120  to  9150\n",
      "Batch:  9150  to  9180\n",
      "Batch:  9180  to  9210\n",
      "Batch:  9210  to  9240\n",
      "Batch:  9240  to  9270\n",
      "Batch:  9270  to  9300\n",
      "Batch:  9300  to  9330\n",
      "Batch:  9330  to  9360\n",
      "Batch:  9360  to  9390\n",
      "Batch:  9390  to  9420\n",
      "Batch:  9420  to  9450\n",
      "Batch:  9450  to  9480\n",
      "Batch:  9480  to  9510\n",
      "Batch:  9510  to  9540\n",
      "Batch:  9540  to  9570\n",
      "Batch:  9570  to  9600\n",
      "Batch:  9600  to  9630\n",
      "Batch:  9630  to  9660\n",
      "Batch:  9660  to  9690\n",
      "Batch:  9690  to  9720\n",
      "Batch:  9720  to  9750\n",
      "Batch:  9750  to  9780\n",
      "Batch:  9780  to  9810\n",
      "Batch:  9810  to  9840\n",
      "Batch:  9840  to  9870\n",
      "Batch:  9870  to  9900\n",
      "Batch:  9900  to  9930\n",
      "Batch:  9930  to  9960\n",
      "Batch:  9960  to  9990\n",
      "Batch:  9990  to  10020\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "batch_size = 30\n",
    "responses = []  # List to store all responses\n",
    "\n",
    "# Process the dataset in batches\n",
    "for i in range(0, len(test_dataset['question']), batch_size):\n",
    "    print(\"Batch: \", i,\" to \", i+batch_size)\n",
    "    batch_questions = test_dataset['question'][i:i+batch_size]\n",
    "    batch_answers = test_dataset['answer'][i:i+batch_size]\n",
    "    batch_solutions = test_dataset['solution'][i:i+batch_size]\n",
    "\n",
    "    # Generate input prompts for the batch\n",
    "    input_prompts = [\n",
    "        prompt.format(q, a, s, \"\")\n",
    "        for q, a, s in zip(batch_questions, batch_answers, batch_solutions)\n",
    "    ]\n",
    "\n",
    "    # Tokenize inputs as a batch\n",
    "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    input_token_lens = inputs['input_ids'].shape[1]  # Total length of the input tokens\n",
    "\n",
    "    # Generate outputs for the entire batch\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "    batch_responses = [\n",
    "        tokenizer.decode(outputs[j][input_token_lens:], skip_special_tokens=True).strip()\n",
    "        for j in range(outputs.size(0))\n",
    "    ]\n",
    "    responses.extend(batch_responses)  # Append batch responses to the main list\n",
    "\n",
    "    # Clean up GPU memory after processing each batch\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        clear_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUHq2cmJsYDK",
    "outputId": "aabdf7e6-d505-43dc-ff73-4428e53a0c8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHhWyETvWuqF",
    "outputId": "0d20028d-9d02-446e-bac9-7fb707be63c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses have been saved to model_responses.csv.\n"
     ]
    }
   ],
   "source": [
    "# Saving responses in a dataframe\n",
    "responses_df = pd.DataFrame({\n",
    "    'ID': range(len(responses)),\n",
    "    'is_correct': responses\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "responses_df.to_csv('model_responses.csv', index=False)\n",
    "print(\"Responses have been saved to model_responses.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKt3vZoSeRvb"
   },
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRiW2RQ0cWru",
    "outputId": "8be41abf-1d40-4069-fdaa-ba877720b28f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local saving\n",
    "model.save_pretrained(\"lora_model\") \n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "cf1d8e9550fa44428588817252c6aa28",
      "7856e7e6ccc24797a9d72b5defeb2305",
      "f14219f87ccf4ae1a3efdbbf27140f76",
      "1e1a608ed6c241488b6cb1253bf6fe69",
      "58c0f60ab2fd47028da9993e32530d4b",
      "a2895ce3b52148e49e49e8db39a9f535",
      "7f40fe3277d04a23a8f21704ace4b3db",
      "2e15ac45ad67436f93ecfa9aa92c42bc",
      "e6641d30e064409ba4f35a59228bbc93",
      "eb22f2a13eb043f9806eac3b32781356",
      "009029dd686e4a55a668ec26cebf4c28"
     ]
    },
    "id": "paHfJLfVccmN",
    "outputId": "b6dd8d43-d28c-46d0-b06c-b7a2044c69c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1390: UserWarning: Current model requires 553652224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1d8e9550fa44428588817252c6aa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", #Trained model\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF1RNRbUeAJy"
   },
   "source": [
    "## Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekPCtZ5XeCQI",
    "outputId": "b3be285d-5042-45ad-a997-d4b9711903dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijTRSGE9feTl"
   },
   "source": [
    "### Saving Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw8V9-Vjfgla"
   },
   "outputs": [],
   "source": [
    "# Define source and destination paths\n",
    "source_file = '/content/model_responses.csv'\n",
    "destination_file = '/content/drive/My Drive/DL/model_responses_85.10.csv'  # Adjust the path where you want to save in Drive\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_file, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Usu0J2JDftO5",
    "outputId": "b6252215-fa27-42f3-f4e2-24ce8d302d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied successfully to /content/drive/My Drive/DL/model_responses_85.10.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the destination directory exists\n",
    "os.makedirs(os.path.dirname(destination_file), exist_ok=True)\n",
    "\n",
    "# Copy the file\n",
    "shutil.copy(source_file, destination_file)\n",
    "print(f\"File copied successfully to {destination_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE-7U34CfC1y"
   },
   "source": [
    "### Saving Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEVqTSlXeEUr"
   },
   "outputs": [],
   "source": [
    "# Source and destination paths\n",
    "source_folder = '/content/outputs'  \n",
    "destination_folder = '/content/drive/My Drive/DL/dropout_0.1/checkpoints3'  \n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he9KP7o6eSgy"
   },
   "outputs": [],
   "source": [
    "# Copy files from source to destination\n",
    "if os.path.exists(source_folder):\n",
    "    # If destination folder already exists, it will throw an error, so check if it's not there\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "else:\n",
    "    print(f\"Source folder {source_folder} does not exist. Please check the source path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EKNwt2zfE2u"
   },
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djLEjvumfSgj"
   },
   "outputs": [],
   "source": [
    "# Define source and destination paths\n",
    "source_folder = '/content/lora_model'  \n",
    "destination_folder = '/content/drive/My Drive/DL/lora_model_dropout_0.1_3' \n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3PI4G3-eVKH",
    "outputId": "c45ca617-8dbc-4c85-d239-c19950063886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source folder /content/lora_model does not exist. Please check the source path.\n"
     ]
    }
   ],
   "source": [
    "# Copy files from source to destination\n",
    "if os.path.exists(source_folder):\n",
    "    # If destination folder already exists, it will throw an error, so check if it's not there\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "else:\n",
    "    print(f\"Source folder {source_folder} does not exist. Please check the source path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZSiGKx-hZBh"
   },
   "source": [
    "## Loading Checkpoint for further training\n",
    "\n",
    "Since env running out of CUDA memory, restarting sessions is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc1nh6Q7ywkf"
   },
   "source": [
    "## Fetching Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEeN7w-2oHzW",
    "outputId": "2d41ecd9-e57d-4c9e-fe39-db5d3af84b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_-7nlU3pEfd"
   },
   "outputs": [],
   "source": [
    "# Define source and destination paths\n",
    "source_folder = '/content/drive/My Drive/DL/dropout_0.1/checkpoints3'\n",
    "destination_folder = '/content/outputs'\n",
    "\n",
    "# Copy files from source to destination\n",
    "if os.path.exists(source_folder):\n",
    "    # If destination folder already exists, it will throw an error, so check if it's not there\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "else:\n",
    "    print(f\"Source folder {source_folder} does not exist. Please check the source path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "0d074e73f7d24d4b9f6195807cda310d",
      "7fd2ceec99974a0dbb67e4dbf90e3413",
      "36f9eb4f08e542149d3b4e550a5279d2",
      "802495d5593547aa857987c4f6c14a34",
      "2a4d790c8e3d4f5ca0e62e36597522ef",
      "02dc469d62cb410c92f506d75e7a2d57",
      "55b10f055e2643d8bdebe3850e70ee62",
      "a8975de63d7648aab3ead864d0980c7a",
      "623e581a05e844748dbec648aab485ed",
      "3c044a169916448aa6469b5dfb2484df",
      "60f957132f964fe78cbd6ff49b449728",
      "81268e9b2716455e9b52e20436bd0376",
      "25393cf74c7449f7816faeae0f8b3507",
      "ab9d7ba8256f4dd0b5ad3c0616f4bafa",
      "6fa9053894c9420ca880e72c492d8652",
      "c117a2da19af4b8fb222b1324e6cb9f5",
      "1e9fb9e8b69443c798ef26a114c92423",
      "657377de3a884690b8b858eae755898e",
      "1080874d48c548609926299e824b3e58",
      "f428d99741c9467fbe38b8a5361f91dd",
      "bcaad1b2b45a4e14a43aaeeba11d8b58",
      "2159b2a624af40fab80373f9d27dd980"
     ]
    },
    "id": "0u084vtafYmm",
    "outputId": "bc9ec4a2-c488-45ef-9781-f8aaeb99f978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d074e73f7d24d4b9f6195807cda310d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81268e9b2716455e9b52e20436bd0376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model from the local copy\n",
    "path_to_checkpoint = os.path.join(destination_folder, \"checkpoint-bestmodel\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8O6zpEZ2rK2"
   },
   "source": [
    "## One point Inference to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHPro5X2y8oI"
   },
   "source": [
    "## SFT Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KdCB4BUvKGp",
    "outputId": "91a3f818-2af4-49df-d75d-b340185b67b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing CUDA memory...\n",
      "CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Clearing CUDA memory...\")\n",
    "    clear_cuda()\n",
    "    print(\"CUDA memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-d-EukamSoI"
   },
   "outputs": [],
   "source": [
    "# Define training args again\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_steps = 1300\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = int(0.1*max_steps),\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = max_steps,\n",
    "        learning_rate = 4e-5,  # 2e-4, 3e-4, 5e-4\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        # eval_steps=1,  # Evaluate after every logging step; adjust as needed\n",
    "        # eval_strategy=\"steps\",  # Evaluate at each logging step\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.02,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_steps=50,            # Save a checkpoint every 10 steps\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        resume_from_checkpoint=path_to_checkpoint,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "2715ca2618a1441a81fec574280988e8",
      "a501b0e49e78470582f029187b3ccc0a",
      "94025edd133b43c89988b654e271805a",
      "9e7b824a30e4416ca7f2ba689e451f2f",
      "79f2d3eca0384231aee937f9a8191481",
      "e4105092f725488787003113cffa7ce5",
      "8b074d605c97402c904b9bc635603d10",
      "a1f8fb8953094720b82e9d4ffc012f8d",
      "2229bd74d87f4d609bd9184465cb45a5",
      "d0be7a0901ef4631be54688d21d70620",
      "1c8297d20ea44b41a26e637f70bd0d43",
      "5d8efd394f6147c9b0d8cc102810ecc5",
      "2b2e70d78d5a49fb9744ef7f1069fcd5",
      "5bc84f8e3d0e454f85e6c7105baee009",
      "ff5399e9eec54802b57085b8e47ee971",
      "1a7dc1bb56d741328388f208d8687712",
      "08200847921d44798c2723fbed083bd1",
      "6b6f750b04a94963988f332ba4775828",
      "19c571d40d0b4b53950fdbf27f120405",
      "3115d47ca84d456ea22c52c4e3acdabe",
      "fd09f1e1acbd4d6499dac27a1a19efc0",
      "37f5f58d72f04267b4b30adfab7876c6"
     ]
    },
    "id": "jJPFA39WmSkv",
    "outputId": "ed733dde-3209-4488-959b-cd0bd7514871"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2715ca2618a1441a81fec574280988e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/995000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8efd394f6147c9b0d8cc102810ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,     #.select(range(2000)),\n",
    "    eval_dataset = eval_dataset,       #.select(range(2000)),\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "99a60484391741529e734730d687869b",
      "7909b90bde544eda9313a6d648551063",
      "f3fd028eb842472ab80ca3d2e75137fb",
      "1df9754ef63e4f73b8973de64ee69389",
      "057550bf74704556859c984993c12629",
      "3a87a0a8de11426e992202eacf3fb4be",
      "b9774b307f9549528bfce4a1497a50e3",
      "63432b39b17443d38c23c2b36ea5eaaf",
      "84a1ad2aacdf4780a5da4a289ca17632",
      "cda2ed0c24ca43fc905be739890ea856",
      "a291230b2be449759533141f7bc07638"
     ]
    },
    "id": "dBEKxJUbirid",
    "outputId": "1c6d728c-4b8c-4dd2-da0a-4be277bf4b51"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 32 | Total steps = 1,300\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1221' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1221/1300 3:59:34 < 45:10, 0.03 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>0.508793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.517694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.522682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.525196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.526424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.526008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.528223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.525705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.524102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.523184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.521323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.521209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.519674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.744900</td>\n",
       "      <td>0.518431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.515613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.514261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.512129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.509959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.508673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.506217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.503962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.501684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.499315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.496332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.494110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.492270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.490112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.487080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.485403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.483374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.481426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.478725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.477143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.475115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.472862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.471295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.469225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.467789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.466013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.463860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>0.461892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.460126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a60484391741529e734730d687869b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/926 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1300 4:50:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>0.508793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.517694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.522682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.525196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.526424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.526008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.528223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.525705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.524102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.523184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.521323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.521209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.519674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.744900</td>\n",
       "      <td>0.518431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.515613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.514261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.512129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.509959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.508673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.506217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.503962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.501684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.499315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.496332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.494110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.492270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.490112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.487080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.485403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.483374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.481426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.478725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.477143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.475115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.472862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.471295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.469225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.467789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.466013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.463860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>0.461892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.460126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.459155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.457691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.456234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.455053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.454109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.453470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.453013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.471700</td>\n",
       "      <td>0.452874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prNoHvvz1ymz"
   },
   "outputs": [],
   "source": [
    "def save_model_checkpoint(trainer, tokenizer, training_args, output_dir):\n",
    "    \"\"\"\n",
    "    Save a complete training checkpoint for SFTTrainer including RNG states and trainer state\n",
    "\n",
    "    Args:\n",
    "        trainer: SFTTrainer instance\n",
    "        tokenizer: The tokenizer to save\n",
    "        training_args: Training arguments/config to save\n",
    "        output_dir: Directory to save the checkpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the model state\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "        # Save the tokenizer\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # Save optimizer state\n",
    "        if trainer.optimizer is not None:\n",
    "            torch.save(trainer.optimizer.state_dict(),\n",
    "                      os.path.join(output_dir, 'optimizer.pt'))\n",
    "\n",
    "        # Save scheduler state\n",
    "        if hasattr(trainer, 'lr_scheduler') and trainer.lr_scheduler is not None:\n",
    "            torch.save(trainer.lr_scheduler.state_dict(),\n",
    "                      os.path.join(output_dir, 'scheduler.pt'))\n",
    "\n",
    "        # Save RNG states\n",
    "        rng_states = {\n",
    "            'python': random.getstate(),\n",
    "            'numpy': np.random.get_state(),\n",
    "            'torch': torch.get_rng_state(),\n",
    "            'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None\n",
    "        }\n",
    "        torch.save(rng_states, os.path.join(output_dir, 'rng_state.pth'))\n",
    "\n",
    "        # Save trainer state\n",
    "        trainer_state = {\n",
    "            'best_metric': trainer.state.best_metric,\n",
    "            'best_model_checkpoint': trainer.state.best_model_checkpoint,\n",
    "            'epoch': trainer.state.epoch,\n",
    "            'global_step': trainer.state.global_step,\n",
    "            'log_history': trainer.state.log_history,\n",
    "            'total_flos': trainer.state.total_flos,\n",
    "            'trial_name': trainer.state.trial_name,\n",
    "            'trial_params': trainer.state.trial_params\n",
    "        }\n",
    "        json.dump(trainer_state,\n",
    "                 open(os.path.join(output_dir, 'trainer_state.json'), 'w'),\n",
    "                 indent=2)\n",
    "\n",
    "        # Save training arguments\n",
    "        if training_args is not None:\n",
    "            torch.save(training_args,\n",
    "                      os.path.join(output_dir, 'training_args.bin'))\n",
    "\n",
    "        print(f\"Checkpoint saved successfully to {output_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKNrbxOI1ylE",
    "outputId": "164824ed-c245-4048-d989-13bf0b8a2057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully to outputs/checkpoint-bestmodel\n"
     ]
    }
   ],
   "source": [
    "save_model_checkpoint(\n",
    "    trainer=trainer,\n",
    "    tokenizer=tokenizer,\n",
    "    training_args=training_args,\n",
    "    output_dir=\"outputs/checkpoint-bestmodel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D2QgS2ytHmP"
   },
   "source": [
    "## Train model in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkfiGqMVtOnD"
   },
   "outputs": [],
   "source": [
    "def train_model_in_chunks(trainer, total_steps, chunk_size=200):\n",
    "    steps_completed = 0\n",
    "    checkpoint_exists = os.path.exists(os.path.join(trainer.args.output_dir, 'checkpoint'))\n",
    "\n",
    "    while steps_completed < total_steps:\n",
    "        if steps_completed == 0 and not checkpoint_exists:\n",
    "            # Initial training without resuming\n",
    "            trainer.train()\n",
    "        else:\n",
    "            # Resume from the last checkpoint\n",
    "            trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "        steps_completed += chunk_size\n",
    "\n",
    "        print(f\"Completed {steps_completed}/{total_steps} steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "af0a3cf0e89f477080afade5d63e3442",
      "47be854732af49f59bde365f746db7a1",
      "f0ca49668b7d41a3887074d33c867c69",
      "8bc92de08b0a46cdbc02547a0ab28c81",
      "82e1e5fb715240f5b3b07493bac5cfae",
      "48f935c0e6ad4440a9e1777da7dd81af",
      "88d5ba40d4bf44eab66eb084bff8be06",
      "f74baa38ea2341868be5eaaecc904056",
      "5455d129db2947d1953fbc39b62dd0db",
      "d141d20b43f54e39896bddf0afbf1ad5",
      "3d44a55226d74effb549c6f12e7fea9b"
     ]
    },
    "id": "1EG_upq0tOkz",
    "outputId": "7c0c687b-46a7-4bda-e818-b16534dcbd20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0a3cf0e89f477080afade5d63e3442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/995000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 6,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 400,\n",
    "        learning_rate = 3e-4,  # 2e-4, 3e-4, 5e-4\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        # eval_steps=1,  # Evaluate after every logging step; adjust as needed\n",
    "        # eval_strategy=\"steps\",  # Evaluate at each logging step\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OL0GbZ1AtHZs",
    "outputId": "0b251ef4-a43a-44fe-f638-08288d6b8eda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 14:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.587700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.598100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.641300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 200/1000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.517200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 400/1000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='202' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 600/1000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='203' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.608700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 800/1000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 995,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 6\n",
      "\\        /    Total batch size = 24 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.614000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1000/1000 steps\n"
     ]
    }
   ],
   "source": [
    "train_model_in_chunks(trainer, total_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
